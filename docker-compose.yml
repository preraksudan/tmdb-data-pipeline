# #version: "3.9"
# services:

#   minio:
#     image: minio/minio:latest
#     container_name: minio
#     env_file:
#       - .env
#     command: server /data --console-address ":9001"
#     ports:
#       - "9000:9000"
#       - "9001:9001"
#     volumes:
#       - minio_data:/data

#   spark:
#     image: apache/spark:3.5.1-scala2.12-java17-python3-ubuntu
#     container_name: spark
#     env_file:
#       - .env
#     command: [ "tail", "-f", "/dev/null" ]
#     volumes:
#       - ./spark:/opt/spark-apps
#     depends_on:
#       - minio

#   postgres:
#     image: postgres:15
#     container_name: airflow-postgres
#     env_file:
#       - .env
#     volumes:
#       - airflow_pgdata:/var/lib/postgresql/data
#     healthcheck:
#       test: [ "CMD", "pg_isready", "-U", "airflow" ]
#       interval: 5s
#       retries: 5

#   airflow:
#     image: apache/airflow:2.9.0
#     container_name: airflow
#     env_file:
#       - .env

#     depends_on:
#       postgres:
#         condition: service_healthy
#       minio:
#         condition: service_started
#       spark:
#         condition: service_started

#     ports:
#       - "8081:8080"

#     volumes:
#       - ./airflow/dags:/opt/airflow/dags
#       - ./spark:/opt/spark-apps

#     command: >
#       bash -c " airflow db migrate && airflow users create
#         --username admin
#         --password admin
#         --firstname admin
#         --lastname admin
#         --role Admin
#         --email admin@example.com || true &&
#       airflow scheduler & airflow webserver "

# volumes:
#   minio_data:
#   airflow_pgdata:

services:

  minio:
    image: minio/minio:latest
    container_name: minio
    env_file:
      - .env
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data

  spark:
    image: apache/spark:3.5.1-scala2.12-java17-python3-ubuntu
    container_name: spark
    hostname: spark
    env_file:
      - .env
    command: [ "tail", "-f", "/dev/null" ]
    volumes:
      - ./spark:/opt/spark-apps
    depends_on:
      - minio

  postgres:
    image: postgres:15
    container_name: airflow-postgres
    env_file:
      - .env
    volumes:
      - airflow_pgdata:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD", "pg_isready", "-U", "${POSTGRES_USER}" ]
      interval: 5s
      retries: 5

  airflow:
    image: apache/airflow:2.9.0
    container_name: airflow
    env_file:
      - .env

    depends_on:
      postgres:
        condition: service_healthy
      spark:
        condition: service_started
    ports:
      - "8081:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./data/raw:/opt/data/raw
      - ./spark:/opt/spark-apps/jobs
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      AIRFLOW__CORE__EXECUTOR: ${AIRFLOW__CORE__EXECUTOR}
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: ${AIRFLOW__DATABASE__SQL_ALCHEMY_CONN}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    entrypoint: >
      /bin/bash -c " pip install apache-airflow-providers-apache-spark==4.7.1 && airflow db upgrade && airflow users create
        --username admin
        --password admin
        --firstname Admin
        --lastname User
        --role Admin
        --email admin@example.com || true &&
      airflow scheduler & airflow webserver "

volumes:
  minio_data:
  airflow_pgdata:
