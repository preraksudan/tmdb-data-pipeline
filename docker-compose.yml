services:
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    env_file: .env
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
  # Service renamed to 'spark' to fix the 'depends_on' error
  # spark:
  #   image: apache/spark:4.1.0-preview4-scala2.13-java21-python3-r-ubuntu
  #   container_name: spark
  #   env_file: .env
  #   # A 'tail -f /dev/null' keeps the container running so airflow can interact with it
  #   command: [ "tail", "-f", "/dev/null" ]
  #   volumes:
  #     - ./spark:/opt/spark-apps
  #   depends_on:
  #     - minio

  spark:
    image: apache/spark:3.5.1-scala2.12-java17-python3-ubuntu
    container_name: spark
    env_file: .env
    command: [ "tail", "-f", "/dev/null" ]
    volumes:
      - ./spark:/opt/spark-apps
    depends_on:
      - minio

  airflow:
    image: apache/airflow:3.1.5
    container_name: airflow
    env_file: .env
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./spark:/opt/spark-apps
    ports:
      - "8080:8080"
    # Recommended user setting for volume permissions
    user: "${AIRFLOW_UID:-50000}:0"
    # Install amazon providers within the container start-up script
    command: >
      bash -c "pip install apache-airflow-providers-amazon && 
               airflow db init && 
               airflow users create
                --username admin
                --password admin
                --firstname admin
                --lastname admin
                --role Admin
                --email admin@example.com &&
               airflow scheduler & 
               airflow webserver "
    depends_on:
      - minio
      # This now correctly references the 'spark' service definition above
      - spark

volumes:
  minio_data:
